{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((15000, 60, 8), (15000,), (16000, 60, 8))"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "# 数据预处理\n",
    "data = np.load('data.npz')\n",
    "train_data = data['train_data']\n",
    "test_data = data['test_data']\n",
    "train_labels = data['train_labels']\n",
    "\n",
    "train_data.shape, train_labels.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.cast(train_data, tf.float32).numpy()\n",
    "train_labels = tf.one_hot(train_labels, 20).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义注意力模块\n",
    "def se_module(seq, reduction_ratio = 16):\n",
    "    #[None, time_steps, channels]\n",
    "    se = keras.layers.GlobalAveragePooling1D()(seq)\n",
    "    #[None, channels]\n",
    "    se = keras.layers.Dense(seq.shape[-1] / reduction_ratio, activation='relu')(se)\n",
    "    se = keras.layers.Dense(seq.shape[-1], activation='relu')(se)\n",
    "    se = tf.nn.sigmoid(se)\n",
    "    #[None, channels]\n",
    "    se = keras.layers.Reshape([1, seq.shape[-1]])(se)\n",
    "    #[None, 1, channels]\n",
    "    return tf.multiply(seq, se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape[1:])\n",
    "    masking = keras.layers.Masking(mask_value=0.0)(inputs)\n",
    "    seq = keras.layers.LSTM(128, return_sequences=True,\n",
    "                                unit_forget_bias=False,\n",
    "                                recurrent_regularizer=keras.regularizers.l2(0.03),\n",
    "    )(masking)\n",
    "    seq = keras.layers.LayerNormalization()(seq)\n",
    "    seq = se_module(seq)\n",
    "    forward_layer = keras.layers.LSTM(128, return_sequences=True,\n",
    "                        unit_forget_bias=False,\n",
    "                        recurrent_regularizer = keras.regularizers.l2(0.015),\n",
    "                        )\n",
    "    backward_layer = keras.layers.LSTM(128, return_sequences=True,\n",
    "                        go_backwards=True,\n",
    "                        unit_forget_bias=False,\n",
    "                        recurrent_regularizer = keras.regularizers.l2(0.005),\n",
    "                        )\n",
    "    seq = keras.layers.Bidirectional(\n",
    "                        layer=forward_layer,\n",
    "                        backward_layer=backward_layer,\n",
    "                        )(seq)\n",
    "    seq = keras.layers.GlobalAveragePooling1D()(seq)\n",
    "    seq = keras.layers.Dense(512, activation='relu')(seq)\n",
    "    seq = keras.layers.Dropout(0.3)(seq)\n",
    "    seq = keras.layers.Dense(128, activation='relu')(seq)\n",
    "    seq = keras.layers.Dropout(0.3)(seq)\n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(seq)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(),\n",
    "            loss=tf.losses.CategoricalCrossentropy(label_smoothing=0.1),           \n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler_func(epoch):\n",
    "    if epoch < 15:\n",
    "        rate = 1e-3\n",
    "    elif epoch >= 15 and epoch < 25:\n",
    "        rate = 3e-4\n",
    "    elif epoch >= 25:\n",
    "        rate = 9e-5\n",
    "    return rate * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99 - val_loss: 1.1214 - val_accuracy: 0.8133 - lr: 9.0000e-05\nEpoch 33/40\n446/446 - 9s - loss: 0.7220 - accuracy: 0.9802 - val_loss: 1.1170 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 34/40\n446/446 - 9s - loss: 0.7224 - accuracy: 0.9789 - val_loss: 1.1223 - val_accuracy: 0.8120 - lr: 9.0000e-05\nEpoch 35/40\n446/446 - 9s - loss: 0.7193 - accuracy: 0.9803 - val_loss: 1.1171 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 36/40\n446/446 - 9s - loss: 0.7176 - accuracy: 0.9815 - val_loss: 1.1180 - val_accuracy: 0.8187 - lr: 9.0000e-05\nEpoch 37/40\n446/446 - 9s - loss: 0.7167 - accuracy: 0.9807 - val_loss: 1.1221 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 38/40\n446/446 - 9s - loss: 0.7157 - accuracy: 0.9823 - val_loss: 1.1177 - val_accuracy: 0.8147 - lr: 9.0000e-05\nEpoch 39/40\n446/446 - 9s - loss: 0.7127 - accuracy: 0.9824 - val_loss: 1.1174 - val_accuracy: 0.8187 - lr: 9.0000e-05\nEpoch 40/40\n446/446 - 9s - loss: 0.7125 - accuracy: 0.9826 - val_loss: 1.1211 - val_accuracy: 0.8200 - lr: 9.0000e-05\ncheckpoint.best: 0.824000\nProcessing fold: 16 (14250, 750)\nData enhanced (noise) => 28500\nEpoch 1/40\n446/446 - 11s - loss: 2.3712 - accuracy: 0.4464 - val_loss: 1.6380 - val_accuracy: 0.5787 - lr: 0.0010\nEpoch 2/40\n446/446 - 9s - loss: 1.6463 - accuracy: 0.5716 - val_loss: 1.4664 - val_accuracy: 0.6413 - lr: 0.0010\nEpoch 3/40\n446/446 - 9s - loss: 1.4852 - accuracy: 0.6453 - val_loss: 1.3362 - val_accuracy: 0.7000 - lr: 0.0010\nEpoch 4/40\n446/446 - 9s - loss: 1.3772 - accuracy: 0.6935 - val_loss: 1.2803 - val_accuracy: 0.7280 - lr: 0.0010\nEpoch 5/40\n446/446 - 9s - loss: 1.2921 - accuracy: 0.7344 - val_loss: 1.2310 - val_accuracy: 0.7507 - lr: 0.0010\nEpoch 6/40\n446/446 - 9s - loss: 1.2345 - accuracy: 0.7612 - val_loss: 1.2060 - val_accuracy: 0.7653 - lr: 0.0010\nEpoch 7/40\n446/446 - 9s - loss: 1.1824 - accuracy: 0.7807 - val_loss: 1.1827 - val_accuracy: 0.7773 - lr: 0.0010\nEpoch 8/40\n446/446 - 9s - loss: 1.1317 - accuracy: 0.8020 - val_loss: 1.1530 - val_accuracy: 0.7880 - lr: 0.0010\nEpoch 9/40\n446/446 - 9s - loss: 1.0958 - accuracy: 0.8204 - val_loss: 1.1617 - val_accuracy: 0.7733 - lr: 0.0010\nEpoch 10/40\n446/446 - 9s - loss: 1.0591 - accuracy: 0.8377 - val_loss: 1.1410 - val_accuracy: 0.7987 - lr: 0.0010\nEpoch 11/40\n446/446 - 9s - loss: 1.0274 - accuracy: 0.8516 - val_loss: 1.1411 - val_accuracy: 0.7853 - lr: 0.0010\nEpoch 12/40\n446/446 - 9s - loss: 0.9953 - accuracy: 0.8653 - val_loss: 1.1442 - val_accuracy: 0.7920 - lr: 0.0010\nEpoch 13/40\n446/446 - 9s - loss: 0.9686 - accuracy: 0.8804 - val_loss: 1.1246 - val_accuracy: 0.7973 - lr: 0.0010\nEpoch 14/40\n446/446 - 9s - loss: 0.9484 - accuracy: 0.8863 - val_loss: 1.1293 - val_accuracy: 0.7987 - lr: 0.0010\nEpoch 15/40\n446/446 - 9s - loss: 0.9177 - accuracy: 0.8990 - val_loss: 1.1263 - val_accuracy: 0.7960 - lr: 0.0010\nEpoch 16/40\n446/446 - 9s - loss: 0.8410 - accuracy: 0.9318 - val_loss: 1.0708 - val_accuracy: 0.8227 - lr: 3.0000e-04\nEpoch 17/40\n446/446 - 9s - loss: 0.8162 - accuracy: 0.9425 - val_loss: 1.0719 - val_accuracy: 0.8173 - lr: 3.0000e-04\nEpoch 18/40\n446/446 - 9s - loss: 0.8077 - accuracy: 0.9460 - val_loss: 1.0688 - val_accuracy: 0.8200 - lr: 3.0000e-04\nEpoch 19/40\n446/446 - 9s - loss: 0.8011 - accuracy: 0.9481 - val_loss: 1.0774 - val_accuracy: 0.8280 - lr: 3.0000e-04\nEpoch 20/40\n446/446 - 9s - loss: 0.7943 - accuracy: 0.9505 - val_loss: 1.0796 - val_accuracy: 0.8200 - lr: 3.0000e-04\nEpoch 21/40\n446/446 - 9s - loss: 0.7828 - accuracy: 0.9562 - val_loss: 1.0809 - val_accuracy: 0.8147 - lr: 3.0000e-04\nEpoch 22/40\n446/446 - 9s - loss: 0.7782 - accuracy: 0.9578 - val_loss: 1.0767 - val_accuracy: 0.8213 - lr: 3.0000e-04\nEpoch 23/40\n446/446 - 9s - loss: 0.7724 - accuracy: 0.9603 - val_loss: 1.0805 - val_accuracy: 0.8173 - lr: 3.0000e-04\nEpoch 24/40\n446/446 - 9s - loss: 0.7654 - accuracy: 0.9634 - val_loss: 1.0777 - val_accuracy: 0.8213 - lr: 3.0000e-04\nEpoch 25/40\n446/446 - 9s - loss: 0.7596 - accuracy: 0.9655 - val_loss: 1.0929 - val_accuracy: 0.8187 - lr: 3.0000e-04\nEpoch 26/40\n446/446 - 9s - loss: 0.7412 - accuracy: 0.9723 - val_loss: 1.0687 - val_accuracy: 0.8227 - lr: 9.0000e-05\nEpoch 27/40\n446/446 - 9s - loss: 0.7335 - accuracy: 0.9756 - val_loss: 1.0703 - val_accuracy: 0.8173 - lr: 9.0000e-05\nEpoch 28/40\n446/446 - 9s - loss: 0.7334 - accuracy: 0.9756 - val_loss: 1.0753 - val_accuracy: 0.8267 - lr: 9.0000e-05\nEpoch 29/40\n446/446 - 9s - loss: 0.7295 - accuracy: 0.9775 - val_loss: 1.0709 - val_accuracy: 0.8240 - lr: 9.0000e-05\nEpoch 30/40\n446/446 - 9s - loss: 0.7272 - accuracy: 0.9776 - val_loss: 1.0752 - val_accuracy: 0.8187 - lr: 9.0000e-05\nEpoch 31/40\n446/446 - 9s - loss: 0.7264 - accuracy: 0.9779 - val_loss: 1.0768 - val_accuracy: 0.8227 - lr: 9.0000e-05\nEpoch 32/40\n446/446 - 9s - loss: 0.7234 - accuracy: 0.9795 - val_loss: 1.0772 - val_accuracy: 0.8240 - lr: 9.0000e-05\nEpoch 33/40\n446/446 - 9s - loss: 0.7230 - accuracy: 0.9794 - val_loss: 1.0778 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 34/40\n446/446 - 9s - loss: 0.7219 - accuracy: 0.9796 - val_loss: 1.0762 - val_accuracy: 0.8227 - lr: 9.0000e-05\nEpoch 35/40\n446/446 - 9s - loss: 0.7186 - accuracy: 0.9816 - val_loss: 1.0761 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 36/40\n446/446 - 9s - loss: 0.7190 - accuracy: 0.9816 - val_loss: 1.0752 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 37/40\n446/446 - 9s - loss: 0.7172 - accuracy: 0.9819 - val_loss: 1.0828 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 38/40\n446/446 - 9s - loss: 0.7151 - accuracy: 0.9825 - val_loss: 1.0750 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 39/40\n446/446 - 9s - loss: 0.7129 - accuracy: 0.9825 - val_loss: 1.0813 - val_accuracy: 0.8227 - lr: 9.0000e-05\nEpoch 40/40\n446/446 - 9s - loss: 0.7121 - accuracy: 0.9823 - val_loss: 1.0795 - val_accuracy: 0.8200 - lr: 9.0000e-05\ncheckpoint.best: 0.828000\nProcessing fold: 17 (14250, 750)\nData enhanced (noise) => 28500\nEpoch 1/40\n446/446 - 10s - loss: 2.3718 - accuracy: 0.4464 - val_loss: 1.6403 - val_accuracy: 0.5733 - lr: 0.0010\nEpoch 2/40\n446/446 - 9s - loss: 1.6270 - accuracy: 0.5782 - val_loss: 1.4478 - val_accuracy: 0.6547 - lr: 0.0010\nEpoch 3/40\n446/446 - 9s - loss: 1.4778 - accuracy: 0.6504 - val_loss: 1.3449 - val_accuracy: 0.7133 - lr: 0.0010\nEpoch 4/40\n446/446 - 9s - loss: 1.3702 - accuracy: 0.6985 - val_loss: 1.2874 - val_accuracy: 0.7293 - lr: 0.0010\nEpoch 5/40\n446/446 - 9s - loss: 1.2912 - accuracy: 0.7334 - val_loss: 1.2305 - val_accuracy: 0.7493 - lr: 0.0010\nEpoch 6/40\n446/446 - 9s - loss: 1.2306 - accuracy: 0.7605 - val_loss: 1.1695 - val_accuracy: 0.7787 - lr: 0.0010\nEpoch 7/40\n446/446 - 9s - loss: 1.1719 - accuracy: 0.7844 - val_loss: 1.1539 - val_accuracy: 0.7813 - lr: 0.0010\nEpoch 8/40\n446/446 - 9s - loss: 1.1347 - accuracy: 0.8044 - val_loss: 1.1704 - val_accuracy: 0.7840 - lr: 0.0010\nEpoch 9/40\n446/446 - 9s - loss: 1.0962 - accuracy: 0.8205 - val_loss: 1.1622 - val_accuracy: 0.7747 - lr: 0.0010\nEpoch 10/40\n446/446 - 9s - loss: 1.0613 - accuracy: 0.8370 - val_loss: 1.1394 - val_accuracy: 0.7853 - lr: 0.0010\nEpoch 11/40\n446/446 - 9s - loss: 1.0267 - accuracy: 0.8501 - val_loss: 1.1110 - val_accuracy: 0.8040 - lr: 0.0010\nEpoch 12/40\n446/446 - 9s - loss: 0.9985 - accuracy: 0.8660 - val_loss: 1.1142 - val_accuracy: 0.8000 - lr: 0.0010\nEpoch 13/40\n446/446 - 9s - loss: 0.9734 - accuracy: 0.8746 - val_loss: 1.1082 - val_accuracy: 0.8013 - lr: 0.0010\nEpoch 14/40\n446/446 - 9s - loss: 0.9539 - accuracy: 0.8848 - val_loss: 1.1263 - val_accuracy: 0.7960 - lr: 0.0010\nEpoch 15/40\n446/446 - 9s - loss: 0.9220 - accuracy: 0.8978 - val_loss: 1.0989 - val_accuracy: 0.8133 - lr: 0.0010\nEpoch 16/40\n446/446 - 9s - loss: 0.8428 - accuracy: 0.9321 - val_loss: 1.0576 - val_accuracy: 0.8253 - lr: 3.0000e-04\nEpoch 17/40\n446/446 - 9s - loss: 0.8241 - accuracy: 0.9392 - val_loss: 1.0522 - val_accuracy: 0.8253 - lr: 3.0000e-04\nEpoch 18/40\n446/446 - 9s - loss: 0.8130 - accuracy: 0.9448 - val_loss: 1.0566 - val_accuracy: 0.8240 - lr: 3.0000e-04\nEpoch 19/40\n446/446 - 9s - loss: 0.8007 - accuracy: 0.9506 - val_loss: 1.0675 - val_accuracy: 0.8293 - lr: 3.0000e-04\nEpoch 20/40\n446/446 - 9s - loss: 0.7978 - accuracy: 0.9509 - val_loss: 1.0694 - val_accuracy: 0.8227 - lr: 3.0000e-04\nEpoch 21/40\n446/446 - 9s - loss: 0.7890 - accuracy: 0.9547 - val_loss: 1.0665 - val_accuracy: 0.8360 - lr: 3.0000e-04\nEpoch 22/40\n446/446 - 9s - loss: 0.7809 - accuracy: 0.9561 - val_loss: 1.0595 - val_accuracy: 0.8347 - lr: 3.0000e-04\nEpoch 23/40\n446/446 - 9s - loss: 0.7780 - accuracy: 0.9588 - val_loss: 1.0660 - val_accuracy: 0.8280 - lr: 3.0000e-04\nEpoch 24/40\n446/446 - 9s - loss: 0.7695 - accuracy: 0.9624 - val_loss: 1.0585 - val_accuracy: 0.8293 - lr: 3.0000e-04\nEpoch 25/40\n446/446 - 9s - loss: 0.7634 - accuracy: 0.9644 - val_loss: 1.0552 - val_accuracy: 0.8320 - lr: 3.0000e-04\nEpoch 26/40\n446/446 - 9s - loss: 0.7435 - accuracy: 0.9717 - val_loss: 1.0515 - val_accuracy: 0.8293 - lr: 9.0000e-05\nEpoch 27/40\n446/446 - 9s - loss: 0.7366 - accuracy: 0.9751 - val_loss: 1.0516 - val_accuracy: 0.8280 - lr: 9.0000e-05\nEpoch 28/40\n446/446 - 9s - loss: 0.7356 - accuracy: 0.9744 - val_loss: 1.0478 - val_accuracy: 0.8240 - lr: 9.0000e-05\nEpoch 29/40\n446/446 - 9s - loss: 0.7334 - accuracy: 0.9759 - val_loss: 1.0497 - val_accuracy: 0.8253 - lr: 9.0000e-05\nEpoch 30/40\n446/446 - 9s - loss: 0.7303 - accuracy: 0.9777 - val_loss: 1.0509 - val_accuracy: 0.8320 - lr: 9.0000e-05\nEpoch 31/40\n446/446 - 9s - loss: 0.7291 - accuracy: 0.9778 - val_loss: 1.0495 - val_accuracy: 0.8347 - lr: 9.0000e-05\nEpoch 32/40\n446/446 - 9s - loss: 0.7260 - accuracy: 0.9793 - val_loss: 1.0589 - val_accuracy: 0.8267 - lr: 9.0000e-05\nEpoch 33/40\n446/446 - 9s - loss: 0.7241 - accuracy: 0.9792 - val_loss: 1.0592 - val_accuracy: 0.8253 - lr: 9.0000e-05\nEpoch 34/40\n446/446 - 9s - loss: 0.7242 - accuracy: 0.9795 - val_loss: 1.0625 - val_accuracy: 0.8240 - lr: 9.0000e-05\nEpoch 35/40\n446/446 - 9s - loss: 0.7235 - accuracy: 0.9794 - val_loss: 1.0566 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 36/40\n446/446 - 9s - loss: 0.7209 - accuracy: 0.9806 - val_loss: 1.0668 - val_accuracy: 0.8187 - lr: 9.0000e-05\nEpoch 37/40\n446/446 - 9s - loss: 0.7192 - accuracy: 0.9809 - val_loss: 1.0591 - val_accuracy: 0.8253 - lr: 9.0000e-05\nEpoch 38/40\n446/446 - 9s - loss: 0.7183 - accuracy: 0.9815 - val_loss: 1.0534 - val_accuracy: 0.8307 - lr: 9.0000e-05\nEpoch 39/40\n446/446 - 9s - loss: 0.7164 - accuracy: 0.9815 - val_loss: 1.0591 - val_accuracy: 0.8253 - lr: 9.0000e-05\nEpoch 40/40\n446/446 - 9s - loss: 0.7135 - accuracy: 0.9825 - val_loss: 1.0539 - val_accuracy: 0.8280 - lr: 9.0000e-05\ncheckpoint.best: 0.836000\nProcessing fold: 18 (14250, 750)\nData enhanced (noise) => 28500\nEpoch 1/40\n446/446 - 11s - loss: 2.3772 - accuracy: 0.4482 - val_loss: 1.6917 - val_accuracy: 0.5320 - lr: 0.0010\nEpoch 2/40\n446/446 - 9s - loss: 1.6375 - accuracy: 0.5758 - val_loss: 1.5068 - val_accuracy: 0.6013 - lr: 0.0010\nEpoch 3/40\n446/446 - 9s - loss: 1.4753 - accuracy: 0.6487 - val_loss: 1.4088 - val_accuracy: 0.6573 - lr: 0.0010\nEpoch 4/40\n446/446 - 9s - loss: 1.3723 - accuracy: 0.6971 - val_loss: 1.3590 - val_accuracy: 0.6920 - lr: 0.0010\nEpoch 5/40\n446/446 - 9s - loss: 1.2872 - accuracy: 0.7348 - val_loss: 1.3318 - val_accuracy: 0.7107 - lr: 0.0010\nEpoch 6/40\n446/446 - 9s - loss: 1.2257 - accuracy: 0.7634 - val_loss: 1.3254 - val_accuracy: 0.7173 - lr: 0.0010\nEpoch 7/40\n446/446 - 9s - loss: 1.1768 - accuracy: 0.7846 - val_loss: 1.2747 - val_accuracy: 0.7413 - lr: 0.0010\nEpoch 8/40\n446/446 - 9s - loss: 1.1314 - accuracy: 0.8061 - val_loss: 1.2343 - val_accuracy: 0.7427 - lr: 0.0010\nEpoch 9/40\n446/446 - 9s - loss: 1.0898 - accuracy: 0.8244 - val_loss: 1.2528 - val_accuracy: 0.7347 - lr: 0.0010\nEpoch 10/40\n446/446 - 9s - loss: 1.0596 - accuracy: 0.8380 - val_loss: 1.2355 - val_accuracy: 0.7400 - lr: 0.0010\nEpoch 11/40\n446/446 - 9s - loss: 1.0225 - accuracy: 0.8532 - val_loss: 1.2324 - val_accuracy: 0.7507 - lr: 0.0010\nEpoch 12/40\n446/446 - 9s - loss: 0.9917 - accuracy: 0.8678 - val_loss: 1.2150 - val_accuracy: 0.7533 - lr: 0.0010\nEpoch 13/40\n446/446 - 9s - loss: 0.9688 - accuracy: 0.8775 - val_loss: 1.2182 - val_accuracy: 0.7627 - lr: 0.0010\nEpoch 14/40\n446/446 - 9s - loss: 0.9380 - accuracy: 0.8922 - val_loss: 1.1969 - val_accuracy: 0.7693 - lr: 0.0010\nEpoch 15/40\n446/446 - 9s - loss: 0.9197 - accuracy: 0.8987 - val_loss: 1.1898 - val_accuracy: 0.7760 - lr: 0.0010\nEpoch 16/40\n446/446 - 9s - loss: 0.8419 - accuracy: 0.9316 - val_loss: 1.1587 - val_accuracy: 0.7827 - lr: 3.0000e-04\nEpoch 17/40\n446/446 - 9s - loss: 0.8152 - accuracy: 0.9428 - val_loss: 1.1496 - val_accuracy: 0.7893 - lr: 3.0000e-04\nEpoch 18/40\n446/446 - 9s - loss: 0.8050 - accuracy: 0.9479 - val_loss: 1.1524 - val_accuracy: 0.7853 - lr: 3.0000e-04\nEpoch 19/40\n446/446 - 9s - loss: 0.7973 - accuracy: 0.9505 - val_loss: 1.1703 - val_accuracy: 0.7720 - lr: 3.0000e-04\nEpoch 20/40\n446/446 - 9s - loss: 0.7897 - accuracy: 0.9525 - val_loss: 1.1616 - val_accuracy: 0.7840 - lr: 3.0000e-04\nEpoch 21/40\n446/446 - 9s - loss: 0.7822 - accuracy: 0.9550 - val_loss: 1.1687 - val_accuracy: 0.7760 - lr: 3.0000e-04\nEpoch 22/40\n446/446 - 9s - loss: 0.7744 - accuracy: 0.9589 - val_loss: 1.1656 - val_accuracy: 0.7880 - lr: 3.0000e-04\nEpoch 23/40\n446/446 - 9s - loss: 0.7678 - accuracy: 0.9617 - val_loss: 1.1786 - val_accuracy: 0.7760 - lr: 3.0000e-04\nEpoch 24/40\n446/446 - 9s - loss: 0.7662 - accuracy: 0.9634 - val_loss: 1.1735 - val_accuracy: 0.7800 - lr: 3.0000e-04\nEpoch 25/40\n446/446 - 9s - loss: 0.7592 - accuracy: 0.9658 - val_loss: 1.1717 - val_accuracy: 0.7867 - lr: 3.0000e-04\nEpoch 26/40\n446/446 - 9s - loss: 0.7384 - accuracy: 0.9741 - val_loss: 1.1650 - val_accuracy: 0.7800 - lr: 9.0000e-05\nEpoch 27/40\n446/446 - 9s - loss: 0.7335 - accuracy: 0.9751 - val_loss: 1.1674 - val_accuracy: 0.7853 - lr: 9.0000e-05\nEpoch 28/40\n446/446 - 9s - loss: 0.7305 - accuracy: 0.9766 - val_loss: 1.1633 - val_accuracy: 0.7853 - lr: 9.0000e-05\nEpoch 29/40\n446/446 - 9s - loss: 0.7279 - accuracy: 0.9771 - val_loss: 1.1657 - val_accuracy: 0.7880 - lr: 9.0000e-05\nEpoch 30/40\n446/446 - 9s - loss: 0.7278 - accuracy: 0.9770 - val_loss: 1.1650 - val_accuracy: 0.7893 - lr: 9.0000e-05\nEpoch 31/40\n446/446 - 9s - loss: 0.7246 - accuracy: 0.9789 - val_loss: 1.1736 - val_accuracy: 0.7827 - lr: 9.0000e-05\nEpoch 32/40\n446/446 - 9s - loss: 0.7228 - accuracy: 0.9800 - val_loss: 1.1758 - val_accuracy: 0.7867 - lr: 9.0000e-05\nEpoch 33/40\n446/446 - 9s - loss: 0.7215 - accuracy: 0.9793 - val_loss: 1.1733 - val_accuracy: 0.7880 - lr: 9.0000e-05\nEpoch 34/40\n446/446 - 9s - loss: 0.7179 - accuracy: 0.9803 - val_loss: 1.1725 - val_accuracy: 0.7947 - lr: 9.0000e-05\nEpoch 35/40\n446/446 - 9s - loss: 0.7183 - accuracy: 0.9806 - val_loss: 1.1771 - val_accuracy: 0.7867 - lr: 9.0000e-05\nEpoch 36/40\n446/446 - 9s - loss: 0.7166 - accuracy: 0.9814 - val_loss: 1.1791 - val_accuracy: 0.7853 - lr: 9.0000e-05\nEpoch 37/40\n446/446 - 9s - loss: 0.7138 - accuracy: 0.9826 - val_loss: 1.1774 - val_accuracy: 0.7840 - lr: 9.0000e-05\nEpoch 38/40\n446/446 - 9s - loss: 0.7127 - accuracy: 0.9829 - val_loss: 1.1738 - val_accuracy: 0.7880 - lr: 9.0000e-05\nEpoch 39/40\n446/446 - 9s - loss: 0.7128 - accuracy: 0.9826 - val_loss: 1.1802 - val_accuracy: 0.7787 - lr: 9.0000e-05\nEpoch 40/40\n446/446 - 9s - loss: 0.7112 - accuracy: 0.9829 - val_loss: 1.1758 - val_accuracy: 0.7880 - lr: 9.0000e-05\ncheckpoint.best: 0.794667\nProcessing fold: 19 (14250, 750)\nData enhanced (noise) => 28500\nEpoch 1/40\n446/446 - 10s - loss: 2.3789 - accuracy: 0.4446 - val_loss: 1.6819 - val_accuracy: 0.5667 - lr: 0.0010\nEpoch 2/40\n446/446 - 9s - loss: 1.6242 - accuracy: 0.5785 - val_loss: 1.5054 - val_accuracy: 0.6320 - lr: 0.0010\nEpoch 3/40\n446/446 - 9s - loss: 1.4741 - accuracy: 0.6497 - val_loss: 1.4287 - val_accuracy: 0.6600 - lr: 0.0010\nEpoch 4/40\n446/446 - 9s - loss: 1.3730 - accuracy: 0.6929 - val_loss: 1.3092 - val_accuracy: 0.7133 - lr: 0.0010\nEpoch 5/40\n446/446 - 9s - loss: 1.2990 - accuracy: 0.7290 - val_loss: 1.2814 - val_accuracy: 0.7267 - lr: 0.0010\nEpoch 6/40\n446/446 - 9s - loss: 1.2354 - accuracy: 0.7590 - val_loss: 1.2673 - val_accuracy: 0.7160 - lr: 0.0010\nEpoch 7/40\n446/446 - 9s - loss: 1.1819 - accuracy: 0.7828 - val_loss: 1.2474 - val_accuracy: 0.7320 - lr: 0.0010\nEpoch 8/40\n446/446 - 9s - loss: 1.1399 - accuracy: 0.8016 - val_loss: 1.2270 - val_accuracy: 0.7400 - lr: 0.0010\nEpoch 9/40\n446/446 - 9s - loss: 1.0996 - accuracy: 0.8214 - val_loss: 1.1617 - val_accuracy: 0.7733 - lr: 0.0010\nEpoch 10/40\n446/446 - 9s - loss: 1.0608 - accuracy: 0.8382 - val_loss: 1.1663 - val_accuracy: 0.7720 - lr: 0.0010\nEpoch 11/40\n446/446 - 9s - loss: 1.0356 - accuracy: 0.8491 - val_loss: 1.1815 - val_accuracy: 0.7853 - lr: 0.0010\nEpoch 12/40\n446/446 - 9s - loss: 1.0049 - accuracy: 0.8629 - val_loss: 1.1581 - val_accuracy: 0.7827 - lr: 0.0010\nEpoch 13/40\n446/446 - 9s - loss: 0.9695 - accuracy: 0.8799 - val_loss: 1.1569 - val_accuracy: 0.7907 - lr: 0.0010\nEpoch 14/40\n446/446 - 9s - loss: 0.9464 - accuracy: 0.8883 - val_loss: 1.1395 - val_accuracy: 0.8027 - lr: 0.0010\nEpoch 15/40\n446/446 - 9s - loss: 0.9253 - accuracy: 0.8990 - val_loss: 1.1845 - val_accuracy: 0.7920 - lr: 0.0010\nEpoch 16/40\n446/446 - 9s - loss: 0.8424 - accuracy: 0.9328 - val_loss: 1.1013 - val_accuracy: 0.8093 - lr: 3.0000e-04\nEpoch 17/40\n446/446 - 9s - loss: 0.8207 - accuracy: 0.9426 - val_loss: 1.1024 - val_accuracy: 0.8160 - lr: 3.0000e-04\nEpoch 18/40\n446/446 - 9s - loss: 0.8110 - accuracy: 0.9453 - val_loss: 1.1149 - val_accuracy: 0.8133 - lr: 3.0000e-04\nEpoch 19/40\n446/446 - 9s - loss: 0.8054 - accuracy: 0.9453 - val_loss: 1.0996 - val_accuracy: 0.8213 - lr: 3.0000e-04\nEpoch 20/40\n446/446 - 9s - loss: 0.7960 - accuracy: 0.9508 - val_loss: 1.1127 - val_accuracy: 0.8133 - lr: 3.0000e-04\nEpoch 21/40\n446/446 - 9s - loss: 0.7868 - accuracy: 0.9532 - val_loss: 1.1230 - val_accuracy: 0.8093 - lr: 3.0000e-04\nEpoch 22/40\n446/446 - 9s - loss: 0.7819 - accuracy: 0.9564 - val_loss: 1.1227 - val_accuracy: 0.8013 - lr: 3.0000e-04\nEpoch 23/40\n446/446 - 9s - loss: 0.7755 - accuracy: 0.9597 - val_loss: 1.1165 - val_accuracy: 0.8147 - lr: 3.0000e-04\nEpoch 24/40\n446/446 - 9s - loss: 0.7696 - accuracy: 0.9623 - val_loss: 1.1224 - val_accuracy: 0.8147 - lr: 3.0000e-04\nEpoch 25/40\n446/446 - 9s - loss: 0.7630 - accuracy: 0.9641 - val_loss: 1.1414 - val_accuracy: 0.8067 - lr: 3.0000e-04\nEpoch 26/40\n446/446 - 9s - loss: 0.7437 - accuracy: 0.9722 - val_loss: 1.1130 - val_accuracy: 0.8227 - lr: 9.0000e-05\nEpoch 27/40\n446/446 - 9s - loss: 0.7376 - accuracy: 0.9741 - val_loss: 1.1105 - val_accuracy: 0.8173 - lr: 9.0000e-05\nEpoch 28/40\n446/446 - 9s - loss: 0.7361 - accuracy: 0.9752 - val_loss: 1.1085 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 29/40\n446/446 - 9s - loss: 0.7336 - accuracy: 0.9757 - val_loss: 1.1097 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 30/40\n446/446 - 9s - loss: 0.7323 - accuracy: 0.9762 - val_loss: 1.1108 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 31/40\n446/446 - 9s - loss: 0.7305 - accuracy: 0.9765 - val_loss: 1.1040 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 32/40\n446/446 - 9s - loss: 0.7269 - accuracy: 0.9783 - val_loss: 1.1129 - val_accuracy: 0.8253 - lr: 9.0000e-05\nEpoch 33/40\n446/446 - 9s - loss: 0.7274 - accuracy: 0.9783 - val_loss: 1.1199 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 34/40\n446/446 - 9s - loss: 0.7232 - accuracy: 0.9804 - val_loss: 1.1097 - val_accuracy: 0.8293 - lr: 9.0000e-05\nEpoch 35/40\n446/446 - 9s - loss: 0.7246 - accuracy: 0.9792 - val_loss: 1.1170 - val_accuracy: 0.8213 - lr: 9.0000e-05\nEpoch 36/40\n446/446 - 9s - loss: 0.7222 - accuracy: 0.9797 - val_loss: 1.1215 - val_accuracy: 0.8160 - lr: 9.0000e-05\nEpoch 37/40\n446/446 - 9s - loss: 0.7206 - accuracy: 0.9801 - val_loss: 1.1231 - val_accuracy: 0.8133 - lr: 9.0000e-05\nEpoch 38/40\n446/446 - 9s - loss: 0.7179 - accuracy: 0.9811 - val_loss: 1.1210 - val_accuracy: 0.8200 - lr: 9.0000e-05\nEpoch 39/40\n446/446 - 9s - loss: 0.7186 - accuracy: 0.9806 - val_loss: 1.1186 - val_accuracy: 0.8267 - lr: 9.0000e-05\nEpoch 40/40\n446/446 - 9s - loss: 0.7163 - accuracy: 0.9822 - val_loss: 1.1232 - val_accuracy: 0.8200 - lr: 9.0000e-05\ncheckpoint.best: 0.829333\n"
    }
   ],
   "source": [
    "kfcv_fit(builder=lambda : build_model(train_data.shape, 20),\n",
    "                x=train_data, y=train_labels,\n",
    "                epochs=40,\n",
    "                checkpoint_path = './models/lstm/',\n",
    "                batch_size=64,\n",
    "                extra_callbacks=[keras.callbacks.LearningRateScheduler(scheduler_func)],\n",
    "                verbose=2,\n",
    "                noise_std=0.05\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "models/lstm loaded.\nresult got\n"
    }
   ],
   "source": [
    "y_pred = kfcv_predict('models/lstm', test_data)\n",
    "np.save('lstm_20fold (without_feature1).npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_example = pd.read_csv('dataset/submit_example.csv')\n",
    "submit_example['behavior_id'] = np.argmax(y_pred, axis=-1)\n",
    "submit_example.to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           0.7325.csv  0.7453.csv  0.7502.csv  0.7514.csv  0.7524.csv  \\\nacc          0.804375    0.871125    0.770938    0.876687    0.864875   \nacc_combo    0.825500    0.884723    0.793982    0.889286    0.878452   \n\n           0.7566.csv  0.7589.csv  0.7590.csv  0.7620.csv  0.7635.csv  \\\nacc          0.823125    0.839625    0.838375    0.832875    0.831063   \nacc_combo    0.842241    0.855851    0.855854    0.850741    0.849589   \n\n           0.7636.csv  0.7810.csv  \nacc          0.777125    0.877375  \nacc_combo    0.800092    0.890259  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.7325.csv</th>\n      <th>0.7453.csv</th>\n      <th>0.7502.csv</th>\n      <th>0.7514.csv</th>\n      <th>0.7524.csv</th>\n      <th>0.7566.csv</th>\n      <th>0.7589.csv</th>\n      <th>0.7590.csv</th>\n      <th>0.7620.csv</th>\n      <th>0.7635.csv</th>\n      <th>0.7636.csv</th>\n      <th>0.7810.csv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>acc</th>\n      <td>0.804375</td>\n      <td>0.871125</td>\n      <td>0.770938</td>\n      <td>0.876687</td>\n      <td>0.864875</td>\n      <td>0.823125</td>\n      <td>0.839625</td>\n      <td>0.838375</td>\n      <td>0.832875</td>\n      <td>0.831063</td>\n      <td>0.777125</td>\n      <td>0.877375</td>\n    </tr>\n    <tr>\n      <th>acc_combo</th>\n      <td>0.825500</td>\n      <td>0.884723</td>\n      <td>0.793982</td>\n      <td>0.889286</td>\n      <td>0.878452</td>\n      <td>0.842241</td>\n      <td>0.855851</td>\n      <td>0.855854</td>\n      <td>0.850741</td>\n      <td>0.849589</td>\n      <td>0.800092</td>\n      <td>0.890259</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from score_estimator import *\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}