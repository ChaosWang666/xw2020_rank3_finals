{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sensor_train = pd.read_csv('../dataset/sensor_train_final.csv')\n",
    "sensor_test = pd.read_csv('../dataset/sensor_test_final.csv')\n",
    "y = sensor_train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_names = ['time_point','acc_x', 'acc_y', 'acc_z', 'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'acc_g']\n",
    "\n",
    "def handle_features(data):\n",
    "    data['acc'] = (data.acc_x ** 2 + data.acc_y ** 2 + data.acc_z ** 2) ** 0.5\n",
    "    data['acc_g'] = (data.acc_xg ** 2 + data.acc_yg ** 2 + data.acc_zg ** 2) ** 0.5\n",
    "    return data\n",
    "\n",
    "# 重复补齐\n",
    "def handle_mats(grouped_data):\n",
    "    mats = [i.values for i in grouped_data]\n",
    "    max_len = 60\n",
    "    for i in range(len(mats)):\n",
    "        # repeated padding\n",
    "        origin_mat = np.copy(mats[i])\n",
    "        while(True):\n",
    "            padding_size = max_len - len(mats[i])\n",
    "            if padding_size > len(mats[i]):\n",
    "                mats[i] = np.r_[mats[i], origin_mat]\n",
    "            else:\n",
    "                break\n",
    "        # regular padding\n",
    "        if len(mats[i]) < max_len:\n",
    "            padding_size = max_len - len(mats[i])\n",
    "            mats[i] = np.r_[mats[i], np.zeros([padding_size, mats[i].shape[-1]])]\n",
    "        else:\n",
    "            mats[i] = mats[i][:max_len]\n",
    "        mats[i] = mats[i][np.newaxis, :, :]\n",
    "\n",
    "    return np.concatenate(mats, axis=0)\n",
    "\n",
    "train_data = handle_features(sensor_train)\n",
    "test_data = handle_features(sensor_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[src_names] = scaler.fit_transform(train_data[src_names])\n",
    "test_data[src_names] = scaler.transform(test_data[src_names])\n",
    "\n",
    "train_data_grouped = [i.drop(columns='fragment_id') for _, i in train_data.groupby('fragment_id')]\n",
    "train_labels = np.array([int(i.iloc[0]['behavior_id']) for i in train_data_grouped])\n",
    "test_data_grouped = [i.drop(columns='fragment_id') for _, i in test_data.groupby('fragment_id')]\n",
    "\n",
    "for i in range(len(train_data_grouped)):\n",
    "    train_data_grouped[i].drop(columns='behavior_id', inplace=True)\n",
    "\n",
    "x = handle_mats(train_data_grouped)\n",
    "t = handle_mats(test_data_grouped)\n",
    "x = x.reshape((15000, 60, 9, 1))\n",
    "t = t.reshape((16000, 60, 9, 1))\n",
    "\n",
    "x1 = (x[:,:,0,:]).reshape((15000, 60, 1, 1))\n",
    "x = x[:,:,1:9,:].reshape((15000, 60, 4, 2))\n",
    "t1 = (t[:,:,0,:]).reshape((16000, 60, 1, 1))\n",
    "t = t[:,:,1:9,:].reshape((16000, 60, 4, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net():\n",
    "    input1 = Input(shape=(60, 4, 2),name='input1')\n",
    "    input2 = Input(shape=(60, 1, 1),name='input2')\n",
    "    #####################\n",
    "    ###################\n",
    "    X3 = Conv2D(filters=64,kernel_size=(3, 1),activation='relu',padding='same',)(input2)\n",
    "    X3 = Conv2D(filters=8,kernel_size=(3, 1),activation='relu',padding='same',)(X3)\n",
    "    X3 = concatenate([MaxPooling2D(pool_size=(2, 1))(X3),AveragePooling2D(pool_size=(2, 1))(X3)])\n",
    "    X3 = Dropout(0.2)(X3)\n",
    "    ####################\n",
    "    X4 = Conv2D(filters=64,kernel_size=(5, 1),activation='relu',padding='same',)(input2)\n",
    "    X4 = Conv2D(filters=8,kernel_size=(5, 1),activation='relu',padding='same',)(X4)\n",
    "    X4 = concatenate([MaxPooling2D(pool_size=(2, 1))(X4),AveragePooling2D(pool_size=(2, 1))(X4)])\n",
    "    X4 = Dropout(0.2)(X4)\n",
    "    #####################\n",
    "    X5 = Conv2D(filters=64,kernel_size=(7, 1),activation='relu',padding='same',)(input2)\n",
    "    X5 = Conv2D(filters=8,kernel_size=(7, 1),activation='relu',padding='same',)(X5)\n",
    "    X5 = concatenate([MaxPooling2D(pool_size=(2, 1))(X5),AveragePooling2D(pool_size=(2, 1))(X5)])\n",
    "    X5 = Dropout(0.2)(X5)\n",
    "    #####################\n",
    "    X0 = Conv2D(filters=128,kernel_size=(3, 1),activation='relu',padding='same',)(input1+tf.reshape(X3,(-1,60,4,2)))\n",
    "    X0 = Conv2D(filters=128,kernel_size=(3, 3),activation='relu',padding='same',)(X0)\n",
    "    X0 = concatenate([MaxPooling2D()(X0),AveragePooling2D()(X0)])\n",
    "    X0 = Dropout(0.2)(X0)\n",
    "    ####################\n",
    "    X1 = Conv2D(filters=128,kernel_size=(5, 2),activation='relu',padding='same',)(input1+tf.reshape(X4,(-1,60,4,2)))\n",
    "    X1 = Conv2D(filters=128,kernel_size=(5, 3),activation='relu',padding='same',)(X1)\n",
    "    X1 = concatenate([MaxPooling2D()(X1),AveragePooling2D()(X1)])\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "    #####################\n",
    "    X2 = Conv2D(filters=128,kernel_size=(7, 3),activation='relu',padding='same',)(input1+tf.reshape(X5,(-1,60,4,2)))\n",
    "    X2 = Conv2D(filters=128,kernel_size=(7, 3),activation='relu',padding='same',)(X2)\n",
    "    X2 = concatenate([MaxPooling2D()(X2),AveragePooling2D()(X2)])\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    #####################\n",
    "    ######################\n",
    "    X0 = Conv2D(filters=256,kernel_size=(3, 3),activation='relu',padding='same',)(X0)\n",
    "    X0 = Dropout(0.2)(X0)\n",
    "    X0 = Conv2D(filters=512,kernel_size=(3, 3),activation='relu',padding='same',)(X0)\n",
    "    X0 = BatchNormalization()(X0)\n",
    "    X0 = concatenate([GlobalMaxPooling2D()(X0),GlobalAveragePooling2D()(X0)])\n",
    "    ##########\n",
    "    X1 = Conv2D(filters=256,kernel_size=(5, 3),activation='relu',padding='same',)(X1)\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "    X1 = Conv2D(filters=512,kernel_size=(5, 3),activation='relu',padding='same',)(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = concatenate([GlobalMaxPooling2D()(X1),GlobalAveragePooling2D()(X1)])\n",
    "    #########\n",
    "    X2 = Conv2D(filters=256,kernel_size=(7, 3),activation='relu',padding='same',)(X2)\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    X2 = Conv2D(filters=512,kernel_size=(7, 3),activation='relu',padding='same',)(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = concatenate([GlobalMaxPooling2D()(X2),GlobalAveragePooling2D()(X2)])\n",
    "    #########\n",
    "    \n",
    "    X = Dropout(0.3)(concatenate([X0,X1,X2]))\n",
    "    X = Dense(20, activation='softmax')(X)\n",
    "    return Model([input1,input2], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(10, shuffle=True ,random_state=123456)\n",
    "\n",
    "proba_t = np.zeros((16000, 20))\n",
    "#i = 0\n",
    "for fold, (xx, yy) in enumerate(kfold.split(x, y)):\n",
    "    y_ = to_categorical(y, num_classes=20)\n",
    "    model = Net()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['acc'])\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=0,\n",
    "                                mode='max',\n",
    "                                factor=0.5,\n",
    "                                patience=10)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=0,\n",
    "                                   mode='max',\n",
    "                                   patience=30)\n",
    "    checkpoint = ModelCheckpoint('best.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "    model.fit({'input1':x[xx],'input2':x1[xx]}, y_[xx],\n",
    "              epochs=300,\n",
    "              batch_size=32,\n",
    "              verbose=2,\n",
    "              shuffle=True,\n",
    "              validation_data=({'input1':x[yy],'input2':x1[yy]}, y_[yy]),\n",
    "              callbacks=[plateau, early_stopping, checkpoint])\n",
    "    #i +=1\n",
    "    model.load_weights('best.h5')\n",
    "    proba_t += model.predict({'input1':t,'input2':t1}, verbose=0, batch_size=64)/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('conv2d_pred.npy', proba_t)"
   ]
  }
 ]
}