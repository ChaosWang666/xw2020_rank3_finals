{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import models, optimizers, regularizers, initializers,constraints\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/sensor_train_final.csv')\n",
    "test = pd.read_csv('../dataset/sensor_test_final.csv')\n",
    "sub = pd.read_csv('../dataset/submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "data = pd.concat([train, test], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(df):\n",
    "    df = df.astype(\"float\")\n",
    "    try:\n",
    "        tmp = df.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id']]\n",
    "    except:\n",
    "        tmp = df.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id']]\n",
    "\n",
    "    for f in tqdm([f for f in df.columns if 'acc' in f]):\n",
    "        for stat in [\n",
    "                     'min', 'max','median', 'std', 'mean', #\"cid_ce\",\n",
    "#                      'abs_energy',\"absolute_sum_of_changes\",\"cid_ce\",\"count_above_mean\",\"count_below_mean\",\n",
    "#                      \"first_location_of_maximum\",\"first_location_of_minimum\",\"has_duplicate\",\"has_duplicate_max\",\"has_duplicate_min\",\n",
    "#                      \"kurtosis\",\"length\",\"longest_strike_above_mean\",\"longest_strike_below_mean\",\"mean_abs_change\",\"mean_change\",\n",
    "#                      \"mean_second_derivative_central\",\"percentage_of_reoccurring_datapoints_to_all_datapoints\",\"percentage_of_reoccurring_values_to_all_values\",\"ratio_value_number_to_time_series_length\", \"variance_larger_than_standard_deviation\",\n",
    "#                      \"skewness\",\"standard_deviation\",\"sum_of_reoccurring_data_points\",\"sum_of_reoccurring_values\",\"sum_values\",\"variance\",\n",
    "#                      \"augmented_dickey_fuller\",\"last_location_of_maximum\",\"last_location_of_minimum\",\"linear_trend\",\"sample_entropy\",\n",
    "                    ]:\n",
    "            tmp[f+'_'+stat] = df.groupby('fragment_id')[f].agg(stat).values\n",
    "   \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = get_feature(train)\n",
    "feature_train = feature_train.drop([\"fragment_id\",\"behavior_id\"],axis=1)\n",
    "feature_test = get_feature(test)\n",
    "feature_test = feature_test.drop([\"fragment_id\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.shape, feature_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "m_ids = ['acc_x',\"acc_y\",\"acc_z\",'acc_xg',\"acc_yg\",\"acc_zg\"]\n",
    "\n",
    "def get_data(train, m_id):\n",
    "    train[m_id] = train[m_id].apply(lambda x:str(x))\n",
    "    \n",
    "    result = train.groupby(train['fragment_id'])[m_id].agg(\n",
    "    lambda x: x.tolist()\n",
    "    )\n",
    "    return result.tolist()\n",
    "\n",
    "def get_w2v_embedding(m_id, dats=pd.DataFrame()):\n",
    "    path = \"./w2v/w2v_{}_10d.model\".format(m_id)\n",
    "    model_creative_id = Word2Vec.load(path)\n",
    "    vocab_list = [word for word, Vocab in model_creative_id.wv.vocab.items()]# 存储 所有的 词语\n",
    "    word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "    word_vector = {} # 初始化`[word : vector]`字典\n",
    "    embeddings_matrix_creative_id = np.zeros((len(vocab_list) + 1, model_creative_id.vector_size))\n",
    "    for i in range(len(vocab_list)):\n",
    "        word = vocab_list[i] \n",
    "        word_index[word] = i + 1 \n",
    "        word_vector[word] = model_creative_id.wv[word]\n",
    "        embeddings_matrix_creative_id[i + 1] = model_creative_id.wv[word]\n",
    "\n",
    "    if not dats.empty:\n",
    "        sentences = get_data(dats, m_id)\n",
    "        sen2idx = pad_sequences(\n",
    "                                [[word_index.get(str(w), 0) for w in sen] for sen in sentences],\n",
    "                                dtype='int32',\n",
    "                                padding='post',\n",
    "                                maxlen = 60\n",
    "                                )\n",
    "        \n",
    "        return sen2idx\n",
    "    \n",
    "    else:\n",
    "        return embeddings_matrix_creative_id\n",
    "    \n",
    "def trans_dat(x):\n",
    "    w2v_idx = []\n",
    "    for k in m_ids:\n",
    "        item = get_w2v_embedding(k, dats=x)\n",
    "        w2v_idx.append(item)\n",
    "    x = np.concatenate(w2v_idx, axis=1)\n",
    "    return x\n",
    "\n",
    "# window=3 :\n",
    "\n",
    "for m_id in m_ids:\n",
    "    print(\"========\",m_id,\"============\")\n",
    "    sentences = get_data(data, m_id)\n",
    "    w2v_model = Word2Vec(sentences, size=20, iter=64, window=8, seed=2020, sg=1, hs=1)\n",
    "    w2v_model.save(\"./w2v/w2v_{}_10d.model\".format(m_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emb = {k: get_w2v_embedding(k) for k in m_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trans_dat(train)\n",
    "t = trans_dat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bna(x_train, x_test):\n",
    "    features = x_train.columns\n",
    "    for col in features:\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(np.vstack([x_train[[col]].values, x_test[[col]].values]))\n",
    "        x_train[col] = ss.transform(x_train[[col]].values).flatten()\n",
    "        x_test[col] = ss.transform(x_test[[col]].values).flatten()\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(w2v):\n",
    "    w2v_acx = w2v[\"acc_x\"]\n",
    "    w2v_acy = w2v[\"acc_y\"]\n",
    "    w2v_acz = w2v[\"acc_z\"]\n",
    "    w2v_acxg = w2v[\"acc_xg\"]\n",
    "    w2v_acyg = w2v[\"acc_yg\"]\n",
    "    w2v_aczg = w2v[\"acc_zg\"]\n",
    "    \n",
    "    input1 = Input(shape=(60,),name=\"acc_x\")  \n",
    "    input2 = Input(shape=(60,),name=\"acc_y\")     \n",
    "    input3 = Input(shape=(60,),name=\"acc_z\")     \n",
    "    input4 = Input(shape=(60,),name=\"acc_xg\")     \n",
    "    input5 = Input(shape=(60,),name=\"acc_yg\")     \n",
    "    input6 = Input(shape=(60,),name=\"acc_zg\")\n",
    "    feature = Input(shape=(30,),name=\"feature\") \n",
    "    \n",
    "    X1 = Embedding(input_dim=w2v_acx.shape[0],\n",
    "                  output_dim=w2v_acx.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_acx],\n",
    "                  trainable=False)(input1)\n",
    "#     X1 = concatenate([GlobalAveragePooling1D()(X1),GlobalMaxPooling1D()(X1)])\n",
    "    X1 = GlobalAveragePooling1D()(X1)\n",
    "\n",
    "    X2 = Embedding(input_dim=w2v_acy.shape[0],\n",
    "                  output_dim=w2v_acy.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_acy],\n",
    "                  trainable=False)(input2)\n",
    "#     X2 = concatenate([GlobalAveragePooling1D()(X2),GlobalMaxPooling1D()(X2)])\n",
    "    X2 = GlobalAveragePooling1D()(X2)\n",
    "\n",
    "    X3 = Embedding(input_dim=w2v_acz.shape[0],\n",
    "                  output_dim=w2v_acz.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_acz],\n",
    "                  trainable=False)(input3)\n",
    "#     X3 = concatenate([GlobalAveragePooling1D()(X3),GlobalMaxPooling1D()(X3)])\n",
    "    X3 = GlobalAveragePooling1D()(X3)\n",
    "\n",
    "    X4 = Embedding(input_dim=w2v_acxg.shape[0],\n",
    "                  output_dim=w2v_acxg.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_acxg],\n",
    "                  trainable=False)(input4)\n",
    "#     X4 = concatenate([GlobalAveragePooling1D()(X4),GlobalMaxPooling1D()(X4)])\n",
    "    X4 = GlobalAveragePooling1D()(X4)\n",
    "\n",
    "    X5 = Embedding(input_dim=w2v_acyg.shape[0],\n",
    "                  output_dim=w2v_acyg.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_acyg],\n",
    "                  trainable=False)(input5)\n",
    "#     X5 = concatenate([GlobalAveragePooling1D()(X5),GlobalMaxPooling1D()(X5)])\n",
    "    X5 = GlobalAveragePooling1D()(X5)\n",
    "\n",
    "    X6 = Embedding(input_dim=w2v_aczg.shape[0],\n",
    "                  output_dim=w2v_aczg.shape[1],\n",
    "                  input_length=60,\n",
    "                  weights=[w2v_aczg],\n",
    "                  trainable=False)(input6)\n",
    "#     X6 = concatenate([GlobalAveragePooling1D()(X6),GlobalMaxPooling1D()(X6)])\n",
    "    X6 = GlobalAveragePooling1D()(X6)\n",
    "    \n",
    "    X = concatenate([X1,X2,X3,X4,X5,X6, feature])\n",
    "    X = BatchNormalization()(X)\n",
    "    X_len = X.shape[1]\n",
    "    X = Dense(X_len*4, activation=\"relu\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "    X = Dense(X_len*2, activation=\"relu\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "    X = Dense(X_len*1, activation=\"relu\")(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(20, activation=\"softmax\")(X)\n",
    "    \n",
    "    return Model([input1,input2,input3,input4,input5,input6,feature], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "kfold = StratifiedKFold(k, shuffle=True, random_state=256)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "train_pred=np.zeros((15000,20))\n",
    "\n",
    "for fold, (xx, yy) in enumerate(kfold.split(x, y)):\n",
    "    print(\"fold-K: \",fold+1)\n",
    "    y_ = to_categorical(y, num_classes=20)\n",
    "    model = Net(w2v_emb)\n",
    "    model.compile(\n",
    "                optimizer=Adam(),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=['accuracy']\n",
    "                )\n",
    "    callbacks = [\n",
    "                ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 20, verbose = 0, mode = 'max'),\n",
    "                EarlyStopping(monitor = 'val_accuracy', patience = 50, mode = 'max', verbose = 0),\n",
    "#                 ModelCheckpoint(f'./model/fold_{fold}.h5', monitor='val_accuracy',verbose=0,mode='max', save_best_only=True)\n",
    "                ]\n",
    "\n",
    "    model.fit({\n",
    "                \"acc_x\":x[xx,:60],\n",
    "                \"acc_y\":x[xx,60:120],\n",
    "                \"acc_z\":x[xx,120:180],\n",
    "                \"acc_xg\":x[xx,180:240],\n",
    "                \"acc_yg\":x[xx,240:300],\n",
    "                \"acc_zg\":x[xx,300:360],\n",
    "                \"feature\": feature_train.iloc[xx,:]\n",
    "              },\n",
    "              y_[xx,:],\n",
    "              epochs=400,\n",
    "              batch_size=64,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=({\n",
    "                    \"acc_x\":x[yy,:60],\n",
    "                    \"acc_y\":x[yy,60:120],\n",
    "                    \"acc_z\":x[yy,120:180],\n",
    "                    \"acc_xg\":x[yy,180:240],\n",
    "                    \"acc_yg\":x[yy,240:300],\n",
    "                    \"acc_zg\":x[yy,300:360],\n",
    "                    \"feature\": feature_train.iloc[yy,:]\n",
    "              }, y_[yy,:]),\n",
    "              callbacks=callbacks,\n",
    "             )\n",
    "    \n",
    "    train_pred[yy] = model.predict({\n",
    "                    \"acc_x\":x[yy,:60],\n",
    "                    \"acc_y\":x[yy,60:120],\n",
    "                    \"acc_z\":x[yy,120:180],\n",
    "                    \"acc_xg\":x[yy,180:240],\n",
    "                    \"acc_yg\":x[yy,240:300],\n",
    "                    \"acc_zg\":x[yy,300:360],\n",
    "                    \"feature\": feature_train.iloc[yy,:]\n",
    "              }, verbose=0, batch_size=64)\n",
    "    \n",
    "#     model.load_weights(f'./model/fold_{fold}.h5')\n",
    "\n",
    "    proba_t += model.predict({\n",
    "                \"acc_x\":t[:,:60],\n",
    "                \"acc_y\":t[:,60:120],\n",
    "                 \"acc_z\":t[:,120:180],\n",
    "                 \"acc_xg\":t[:,180:240],\n",
    "                 \"acc_yg\":t[:,240:300],\n",
    "                 \"acc_zg\":t[:,300:360],\n",
    "                 \"feature\": feature_test\n",
    "                }, verbose=0, batch_size=128) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dnn_pred.npy', proba_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}