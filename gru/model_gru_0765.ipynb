{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import tensorflow as tf\n",
    "from scipy.fftpack import fft,ifft\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import warnings\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import models, optimizers, regularizers, initializers,constraints\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/sensor_train_final.csv')\n",
    "test = pd.read_csv('../dataset/sensor_test_final.csv')\n",
    "sub = pd.read_csv('../dataset/submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mod'] = (train.acc_x ** 2 + train.acc_y ** 2 + train.acc_z ** 2) ** .5\n",
    "train['modg'] = (train.acc_xg ** 2 + train.acc_yg ** 2 + train.acc_zg ** 2) ** .5\n",
    "test['mod'] = (test.acc_x ** 2 + test.acc_y ** 2 + test.acc_z ** 2) ** .5\n",
    "test['modg'] = (test.acc_xg ** 2 + test.acc_yg ** 2 + test.acc_zg ** 2) ** .5\n",
    "\n",
    "train_fragment_len = len(train[\"fragment_id\"].unique())\n",
    "test_fragment_len = len(test[\"fragment_id\"].unique())\n",
    "\n",
    "x = np.zeros((train_fragment_len, 60, 8))\n",
    "t = np.zeros((test_fragment_len, 60, 8))\n",
    "\n",
    "for i in tqdm(range(15000)):\n",
    "    tmp = train[train.fragment_id == i][:60]\n",
    "    tmp = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],axis=1), 60, np.array(tmp.time_point))[0]\n",
    "    x[i,:,:] = tmp\n",
    "    \n",
    "for i in tqdm(range(16000)):\n",
    "    tmp = test[test.fragment_id == i][:60]\n",
    "    tmp = resample(tmp.drop(['fragment_id', 'time_point'],axis=1), 60, np.array(tmp.time_point))[0]\n",
    "    t[i,:,:] = tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"init\":self.init,\n",
    "            \"supports_masking\":self.supports_masking,\n",
    "            \"W_regularizer\":self.W_regularizer,\n",
    "            \"b_regularizer\":self.b_regularizer,\n",
    "            \"W_constraint\":self.W_constraint,\n",
    "            \"b_constraint\":self.b_constraint,\n",
    "            \"bias\":self.bias,\n",
    "            \"step_dim\":self.step_dim,\n",
    "            \"features_dim\":self.features_dim,\n",
    "        }\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net():\n",
    "    input1 = Input(shape=(60, 8),name=\"rnn\")\n",
    "\n",
    "    x1 = Bidirectional(GRU(128,activation=\"tanh\", return_sequences=True), merge_mode='concat')(input1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "\n",
    "    x2 = Bidirectional(GRU(128,activation=\"tanh\", return_sequences=True), merge_mode='concat')(x1)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x3 = Bidirectional(GRU(128,activation=\"tanh\", return_sequences=True), merge_mode='concat')(x2)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    X = concatenate([x1,x2,x3])\n",
    "    \n",
    "    X = Attention(60)(X)\n",
    "    X_L = X.shape[1]\n",
    "    X = Dense(X_L*4, activation=\"relu\")(X)\n",
    "    X = Dense(X_L*1, activation=\"relu\")(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(20, activation=\"softmax\")(X)\n",
    "\n",
    "    return Model([input1], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "kfold = StratifiedKFold(k, shuffle=True,  random_state=256)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "for fold, (xx, yy) in enumerate(kfold.split(x, y)):\n",
    "    y_ = to_categorical(y, num_classes=20)\n",
    "    model = Net()\n",
    "    model.compile(\n",
    "                optimizer=Adam(),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=['accuracy']\n",
    "                )\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 8,verbose = 0, mode = 'max'),\n",
    "        EarlyStopping(monitor = 'val_accuracy', patience = 18, mode = 'max', verbose = 0),\n",
    "                ]\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    model.fit({\n",
    "                \"rnn\":x[xx]\n",
    "                },\n",
    "              y_[xx],\n",
    "              epochs=150,\n",
    "              batch_size=128,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=({\n",
    "                \"rnn\":x[yy]\n",
    "              }, y_[yy]),\n",
    "              callbacks=callbacks,\n",
    "             )\n",
    "    \n",
    "    proba_t += model.predict({\n",
    "                \"rnn\":t\n",
    "                }, verbose=0, batch_size=128) / k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gru_pred.npy', proba_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}